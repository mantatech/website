[
  {
    "objectID": "index.html#take-care-of-your-data-in-your-infrastructure",
    "href": "index.html#take-care-of-your-data-in-your-infrastructure",
    "title": "manta",
    "section": "Take care of your data in your infrastructure",
    "text": "Take care of your data in your infrastructure\n\n Learning without sharing data  Take advantage of the power of edge computing  Reduce cost of data processing"
  },
  {
    "objectID": "articles/decentralization/index.html",
    "href": "articles/decentralization/index.html",
    "title": "The New Frontier: Data Science in the Decentralized Era",
    "section": "",
    "text": "Data science, in its broadest sense, represents a fusion of multiple disciplines, combining data analysis, statistics, machine learning, and deep learning to extract insights and knowledge from data. Historically, the field of data science has progressed from early methods of recording and analyzing data to the development of complex algorithms and statistical models in the 20th century. The advent of modern computing gave birth to the term “data science” in the 1960s, but it wasn’t until the late 1990s and early 2000s that the field truly exploded, owing to the digital revolution and the exponential growth of data.\nAt its core, data science involves several key concepts: data analysis, machine learning, deep learning, and decision-making algorithms. Data analysis focuses on exploring and visualizing data to find patterns and insights. Machine learning, a subset of artificial intelligence, involves algorithms that enable computers to learn from and make predictions or decisions based on data. Deep learning, a more advanced subset of machine learning, uses neural network architectures to model complex patterns in data. These technologies have been pivotal in advancing numerous fields, such as healthcare for predictive diagnostics, energy management in optimizing consumption, and telecommunications in enhancing network efficiencies.\nOne of the most significant impacts of data science is seen in its application to Internet of Things (IoT) devices. In healthcare, for example, wearable devices gather health metrics, enabling personalized medicine. In smart cities, sensors collect data to optimize traffic flow and reduce energy consumption. These applications, however, have traditionally relied on centralized data processing, where data from various sources are aggregated in a central cloud-based system for analysis.\nThis centralization largely owes its prevalence to the advantages of cloud computing: powerful computational resources, large storage capacities, and centralized control over data and processes. These benefits have shaped the traditional landscape of data science, where massive data sets are processed and analyzed in centralized servers.\nHowever, this centralized approach also brings challenges, particularly in terms of data privacy, security, and sovereignty. The need to transmit data to a central location raises concerns about data breaches and unauthorized access. Additionally, centralization often conflicts with legal and regional regulations regarding data residency and sovereignty. These challenges have led to a growing interest in decentralized data science approaches, where data is processed and analyzed at or near its source, often referred to as “edge computing.” This shift promises enhanced privacy, reduced latency, and compliance with local data governance policies, setting the stage for the next era of data science."
  },
  {
    "objectID": "articles/decentralization/index.html#decentralization-the-why-and-the-how",
    "href": "articles/decentralization/index.html#decentralization-the-why-and-the-how",
    "title": "The New Frontier: Data Science in the Decentralized Era",
    "section": "Decentralization: The Why and The How",
    "text": "Decentralization: The Why and The How\nIn this chapter, we delve into the critical factors prompting a shift from traditional centralized data models to more decentralized approaches in data science. Centralization, while beneficial in many aspects, raises significant concerns regarding data privacy, security, and sovereignty. These issues, coupled with the burgeoning costs associated with cloud computing - both in terms of energy consumption and financial expenditures - highlight the need for change. Addressing these challenges, we explore the emerging paradigms of Edge Computing and Distributed Learning. Edge Computing offers a novel approach by processing data at or near its source, significantly enhancing efficiency and reducing latency. On the other hand, Distributed Learning capitalizes on the collective power of decentralized data sources, enabling more robust and scalable learning solutions. Together, these technologies not only address the limitations of centralization but also unlock new potentials in the realm of data science.\n\nData Privacy, Security, and Sovereignty: The Imperative for Change\nIn the realm of centralized data models, where vast amounts of data are aggregated and processed in cloud-based infrastructures, the challenges of data privacy, security, and sovereignty have become increasingly pronounced. The centralized nature of these systems often makes them attractive targets for malicious activities, leading to significant security concerns. Data breaches in cloud environments have become a notable concern, with recent studies [1] revealing that 39% of businesses experienced a data breach in their cloud environment in the past year. This figure represents a concerning increase from the previous year’s 35%. Such breaches are often attributed to vulnerabilities like cloud misconfigurations or provider-related issues. The implications of these breaches are substantial, not just in terms of immediate data loss or unauthorized access, but also in the time and resources required for recovery. For a significant percentage of businesses, it took more than half a day to restore operations after such breaches.\nPrivacy concerns are further heightened by the increasing scrutiny on how personal data is handled, especially in the wake of high-profile scandals involving the misuse of user data. Regulations such as the European Union’s General Data Protection Regulation [2] have emerged as responses to these concerns, enforcing stricter data protection standards. Addressing data privacy, GDPR stands as a crucial regulatory framework that has significantly influenced data management practices. GDPR mandates stringent data protection standards, challenging traditional data handling methods and pushing for more secure and privacy-focused approaches.\nMoreover, data sovereignty emerges as a crucial issue, particularly for companies bound by national regulations on data storage and processing. The reliance on cloud providers, often located in different jurisdictions, complicates compliance with these regulations. This concern is not just a matter of legal compliance but also touches on national and economic interests, where data is increasingly viewed as a strategic asset.\nGiven these concerns, the centralized cloud-based model, while offering scalability and computational power, faces increasing challenges due to issues surrounding security, privacy, and data sovereignty. These challenges underscore the need for a paradigm shift towards more decentralized approaches in data handling and processing.\n\n\nThe Burgeoning Costs: Energy and Financial Implications of Cloud Computing\nCloud computing, hailed for its scalability, computing power, and high availability, has revolutionized how organizations manage and process data. This technology allows businesses to rapidly scale resources up or down, meeting demand without the need for substantial upfront investment in physical infrastructure. This flexibility, combined with robust computational capabilities and round-the-clock availability, makes cloud computing an attractive option for data storage and processing.\nHowever, this convenience comes at a significant cost, both environmentally and economically. The energy consumption of data centers, key components of cloud infrastructure, has been on a steep rise [3]. From consuming 200 terawatt-hours (TWh) in 2016, it is projected to escalate to an astonishing 2967 TWh by 2030. This sharp increase in energy demand inevitably leads to higher CO2 emissions, especially in regions where the energy mix is heavily reliant on fossil fuels.\nThe trend towards horizontal scaling in cloud computing, while efficient for handling varying workloads, is particularly energy-intensive for complex tasks. Hyperscale data centers, which form the backbone of many cloud services, have seen their energy usage nearly quadruple from 2015 to 2023, becoming the predominant consumers of data center energy globally.\nThe financial implications are equally significant. Despite the challenges posed by the COVID-19 pandemic, the global market for Internet Data Centers is expected to grow from US$59.3 billion in 2020 to US$143.4 billion by 2027, with a compound annual growth rate (CAGR) of 13.4%. This growth, while indicative of the increasing reliance on cloud services, also reflects the rising operational costs associated with these services.\nAnother concerning aspect is server utilization efficiency. Many corporate servers operate at only 10–15% of their capacity, and about 30% are ‘zombie’ servers – inactive yet consuming power. This inefficiency is highlighted by the Power Usage Effectiveness (PUE) metric, which measures the energy efficiency of data centers. In 2021, the average annual PUE was 1.57, indicating a stagnation in improving energy efficiency in data centers over the past years.\nFurthermore, the traditional methods of establishing TCP/IP connections between servers and embedded devices, either through embedded TCP/IP stacks or via gateways, add to the complexity and energy demands of data management.\nEmbedded data centers, which are smaller and less known, consume approximately half of all data center energy in the United States, despite often having less than 50 kW of IT demand. This underlines the widespread energy consumption across various types of data centers, not just the large-scale ones.\nIn light of these figures and trends, it becomes clear that the current trajectory of cloud computing and data center energy consumption is unsustainable. The increasing volume of data and the growing complexity of analytics, such as deeper and more sophisticated neural networks, exacerbate this situation. Therefore, it is imperative to reconsider our approach to data valorization. We must move towards more intelligent, efficient ways of processing and analyzing data, rather than defaulting to the cloud for all computational needs. This shift is not just a matter of optimizing resources; it is a necessary step towards a more sustainable and economically viable future in data science.\n\n\nUnlocking Potential with Edge Computing and Distributed Learning\nAs we navigate the complexities and limitations of cloud-centric data science, particularly the challenges around data privacy, security, sovereignty, and the escalating costs of energy and financial resources, the need for innovative solutions becomes increasingly apparent. This is where Edge Computing and Distributed Learning emerge as pivotal enablers of decentralization, offering new pathways to address these challenges while unlocking the untapped potential of data science.\n\nThe Role of Edge Computing in Decentralization\nEdge Computing has emerged as a transformative force in decentralized data science, driven by significant advancements in the capabilities of devices at the network’s edge. This evolution can be attributed to several key factors:\n\nIncreased Device Power: Modern devices, from smartphones to IoT sensors, have seen remarkable improvements in processing power. This allows them to handle complex computations locally, rather than offloading the task to distant cloud servers.\nEnhanced Storage Capacities: The expansion of storage capabilities in edge devices means more data can be processed and analyzed on-site, reducing the need for constant data transmission to central servers.\nNetwork Performance Improvements: The advent of high-speed networking technologies like 5G has been instrumental. Faster network speeds facilitate quicker data transfer between edge devices and central systems when needed, ensuring that latency is kept to a minimum.\n\nEdge computing has found diverse applications in various fields where real-time data processing is crucial. Some noteworthy examples include:\n\nSmartphones for Next-Word Prediction: Devices like smartphones use edge computing for tasks such as next-word prediction in keyboard apps. This is a classic example of localized data processing that enhances user experience while maintaining data privacy.\nHealthcare Monitoring Devices: Wearable devices that monitor health metrics process data in real-time, providing immediate feedback and alerts. This is crucial for patient monitoring and preventive healthcare.\nSmart City Infrastructure: Edge computing enables smart city technologies to process data from sensors in real-time, optimizing traffic flow, energy usage, and public safety systems without significant delays.\nIndustrial IoT: In manufacturing, edge devices monitor equipment performance, predict maintenance needs, and optimize production processes, directly contributing to increased efficiency and reduced downtime.\n\nWhile edge computing offers numerous benefits, it also presents unique challenges:\n\nResource Availability: Edge devices often serve primary functions other than data processing. Balancing these functions with additional computational tasks can be challenging, especially when resources are limited.\nNetwork Issues: Despite advancements, network connectivity can still be inconsistent, especially in remote or underdeveloped areas. This can impact the performance and reliability of edge computing applications.\nData Management and Security: Managing and securing data across a multitude of edge devices pose significant challenges. Ensuring data integrity and protecting against breaches in such a distributed environment requires robust security protocols.\n\nIn conclusion, edge computing plays a vital role in the decentralization of data science. It leverages the growing capabilities of edge devices and network improvements to bring computation closer to data sources. While offering faster processing, enhanced privacy, and reduced bandwidth usage, it also introduces complexities in resource management and security that need careful consideration\n\n\nThe Power of Distributed Learning\nInitially developed to scale learning processes within datacenters across multiple computers, Distributed Learning was designed to expedite the learning process. This scalability was achieved by parallelizing computations and data processing, thereby reducing the time required for model training. A classic example of this approach is the training of large-scale deep learning models in data centers, where computational tasks are distributed across multiple GPUs or servers to accelerate the training process.\nAs the field evolved, new methods emerged for learning models without centralizing all data in one location. This shift was driven by both practical and privacy considerations. One significant development in this domain is Federated Learning, a subfield of decentralized data science. Federated Learning is inspired by Distributed Learning but takes a novel approach. Instead of aggregating data at a central server, Federated Learning algorithms train models across multiple decentralized devices or servers, each holding its local data. This approach not only preserves data privacy by keeping sensitive information at its source but also reduces the need for data transfer, addressing bandwidth and latency issues.\nHowever, this approach also introduces new challenges, particularly in ensuring model consistency and dealing with non-IID (independently and identically distributed) data across different devices. Additionally, security concerns such as safeguarding against model poisoning and ensuring reliable and secure communication between devices are critical.\nIn summary, Distributed Learning, and specifically Federated Learning, represents a paradigm shift in the way machine learning models are trained. By leveraging the power of distributed systems, it offers a more scalable, efficient, and privacy-preserving approach to machine learning. This evolution not only addresses the limitations of traditional, centralized learning models but also opens up new possibilities for data science applications in a privacy-conscious world."
  },
  {
    "objectID": "about.html#our-team-swimming-into-the-depths-of-innovation",
    "href": "about.html#our-team-swimming-into-the-depths-of-innovation",
    "title": "About us",
    "section": "Our Team : Swimming into the Depths of Innovation",
    "text": "Our Team : Swimming into the Depths of Innovation\n\n\n\n\nHugo Miralles\nCEO\nAdventure enthusiast and leader.Combines a deep technical background with a passion for ethical AI and team building.\n\n\n\n\n\nHugo Miralles\nCTO\nA detail-oriented technologist.Benjamin’s expertise lies in computer science, AI, and distributed computing."
  },
  {
    "objectID": "about.html#our-values",
    "href": "about.html#our-values",
    "title": "About us",
    "section": "Our Values",
    "text": "Our Values\n\n\n\nEdge Efficiency: Smarter Spending for Smarter Data\nManta focuses on redefining data processing economics. We emphasize efficient, cost-effective solutions, leveraging edge computing for economic advantage while maximizing data utility at its source.\n\n\n\nGuardians of Data: Secure, Compliant, Traceable\nOur approach centers on robust data security and adherence to compliance standards like GDPR. We prioritize maintaining data integrity, ensuring security, and offering traceable data management.\n\n\n\nCollaborative Intelligence: Bridging Data Gaps Securely\nManta aims to facilitate secure data collaboration, breaking down corporate silos. Our focus is on fostering secure, innovative partnerships for shared data utilization, enhancing cooperative progress."
  },
  {
    "objectID": "about.html#our-missions",
    "href": "about.html#our-missions",
    "title": "About us",
    "section": "Our Missions",
    "text": "Our Missions\n\n\n\nTrust in every process: Secure Data, Secure Operations\nOur mission is to ensure data security and reliable operations, fostering trust with robust environments where data confidentiality and integrity are paramount.\n\n\n\nScale with simplicity: Extend your World with Decentralization\nWe focus on simplifying scalability in digital environments, offering edge computing solutions for seamless business expansion and resilient decentralized collaboration.\n\n\n\nOperate and Monitor: Command your Digital Frontier\nManta prioritizes empowering users with tools for effective management and oversight of their digital operations, emphasizing intuitive control and monitoring.\n\n\n\nSeamless exploration: Focus on Value, Not Complexity\nOur goal is to streamline data analysis and model training, enabling professionals to focus on deriving value and innovation from their data, free from technical complexities."
  },
  {
    "objectID": "articles.html",
    "href": "articles.html",
    "title": "Articles",
    "section": "",
    "text": "The New Frontier: Data Science in the Decentralized Era\n\n\n\ncloud\n\n\nedge\n\n\ndecentralization\n\n\n\n\n\n\n\n\n\n\nFeb 7, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  }
]